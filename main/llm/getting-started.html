


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
<meta name="robots" content="noindex">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Getting Started with LLMs via ExecuTorch &mdash; ExecuTorch main documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/ExecuTorch-Logo-cropped.svg"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../_static/progress-bar.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Exporting to ExecuTorch Tutorial" href="../tutorials/export-to-executorch-tutorial.html" />
    <link rel="prev" title="Building with CMake" href="../runtime-build-and-cross-compilation.html" />


  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  


  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch">
                  <span class="dropdown-title">ExecuTorch</span>
                </a>
              </div>
            </div>  
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href='https://pytorch.org/executorch/versions.html'>main &#x25BC</a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
    
         
         
         
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro-overview.html">ExecuTorch Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts.html">ExecuTorch Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro-how-it-works.html">How ExecuTorch Works</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting-started-architecture.html">High-level Architecture and Components of ExecuTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started-setup.html">Setting Up ExecuTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime-build-and-cross-compilation.html">Building with CMake</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Working with LLMs</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Getting Started with LLMs via ExecuTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#table-of-contents">Table Of Contents</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="#instantiating-and-executing-an-llm">Instantiating and Executing an LLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quantization-optional">Quantization (Optional)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#debugging-and-profiling">Debugging and Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-use-custom-kernels">How to use custom kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-build-mobile-apps">How to build Mobile Apps</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/export-to-executorch-tutorial.html">Exporting to ExecuTorch Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../running-a-model-cpp-tutorial.html">Running an ExecuTorch Model in C++ Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/sdk-integration-tutorial.html">Using the ExecuTorch SDK to Profile a Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo-apps-ios.html">Building an ExecuTorch iOS Demo App</a></li>
<li class="toctree-l1"><a class="reference internal" href="../demo-apps-android.html">Building an ExecuTorch Android Demo App</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples-end-to-end-to-lower-model-to-delegate.html">Lowering a Model as a Delegate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial-xnnpack-delegate-lowering.html">Building and Running ExecuTorch with XNNPACK Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../executorch-arm-delegate-tutorial.html">Building and Running ExecuTorch with ARM Ethos-U Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build-run-coreml.html">Building and Running ExecuTorch with Core ML Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build-run-mps.html">Building and Running ExecuTorch with MPS Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build-run-qualcomm-ai-engine-direct-backend.html">Building and Running ExecuTorch with Qualcomm AI Engine Direct Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build-run-xtensa.html">Building and Running ExecuTorch on Xtensa HiFi4 DSP</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Exporting to ExecuTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../export-overview.html">Exporting to ExecuTorch</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../export-to-executorch-api-reference.html">Export to ExecuTorch API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../executorch-runtime-api-reference.html">ExecuTorch Runtime API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">IR Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../ir-exir.html">Export IR Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ir-ops-set-definition.html">Definition of the Core ATen Operator Set</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compiler Entry Points</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compiler-delegate-and-partitioner.html">Backend and Delegate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compiler-backend-dialect.html">Backend Dialect</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compiler-custom-compiler-passes.html">Custom Compiler Passes and Partitioners</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compiler-memory-planning.html">Memory Planning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Runtime</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../runtime-overview.html">ExecuTorch Runtime Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime-backend-delegate-implementation-and-linking.html">Backend Delegate Implementation and Linking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime-platform-abstraction-layer.html">Runtime Platform Abstraction Layer (PAL)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../portable-cpp-programming.html">Portable C++ Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pte-file-format.html"><code class="docutils literal notranslate"><span class="pre">.pte</span></code> file format</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quantization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantization-overview.html">Quantization Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Kernel Library</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../kernel-library-overview.html">Overview of ExecuTorch’s Kernel Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kernel-library-custom-aten-kernel.html">Kernel Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kernel-library-selective-build.html">Kernel Library Selective Build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Backend Delegates</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../native-delegates-executorch-xnnpack-delegate.html">ExecuTorch XNNPACK delegate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend-delegates-integration.html">Integrating a Backend Delegate into ExecuTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../backend-delegates-dependencies.html">Third-Party Dependency Management for Backend Delegates</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">SDK</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../sdk-overview.html">Introduction to the ExecuTorch SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sdk-bundled-io.html">Bundled Program – a Tool for ExecuTorch Model Validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sdk-etrecord.html">Prerequisite | ETRecord - ExecuTorch Record</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sdk-etdump.html">Prerequisite | ETDump - ExecuTorch Dump</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sdk-profiling.html">Profiling Models in ExecuTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sdk-debugging.html">Debugging Models in ExecuTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sdk-inspector.html">Inspector APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sdk-delegate-integration.html">SDK Delegate Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sdk-tutorial.html">SDK usage tutorial</a></li>
</ul>

         

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Getting Started with LLMs via ExecuTorch</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/llm/getting-started.md.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        


          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="getting-started-with-llms-via-executorch">
<h1>Getting Started with LLMs via ExecuTorch<a class="headerlink" href="#getting-started-with-llms-via-executorch" title="Permalink to this heading">¶</a></h1>
<div class="section" id="table-of-contents">
<h2>Table Of Contents<a class="headerlink" href="#table-of-contents" title="Permalink to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p>Prerequisites</p></li>
<li><p>Hello World Example</p></li>
<li><p>Quantization</p></li>
<li><p>Using Mobile Acceleration</p></li>
<li><p>Debugging and Profiling</p></li>
<li><p>How to use custom kernels</p></li>
<li><p>How to build mobile apps</p></li>
</ol>
</div>
<div class="section" id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading">¶</a></h2>
<p>Let’s start by getting an ExecuTorch environment:</p>
<ol class="arabic simple">
<li><p>Create a third-party folder (Keeps the file paths organized)</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">mkdir</span>  <span class="n">third</span><span class="o">-</span><span class="n">party</span>
<span class="n">cd</span>  <span class="n">third</span><span class="o">-</span><span class="n">party</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>If you’re new to ExecuTorch follow <a class="reference external" href="https://pytorch.org/executorch/main/getting-started-setup.html#set-up-your-environment">these steps</a> to set up your environment.</p></li>
</ol>
</div>
<div class="section" id="instantiating-and-executing-an-llm">
<h2>Instantiating and Executing an LLM<a class="headerlink" href="#instantiating-and-executing-an-llm" title="Permalink to this heading">¶</a></h2>
<p>We will use Karpathy’s <a class="reference external" href="https://github.com/karpathy/nanoGPT">NanoGPT</a> but you can use another model if you prefer.</p>
<p>There are just 2 steps to this:</p>
<ol class="arabic simple">
<li><p>Export the LLM Model</p></li>
<li><p>Create a runtime to execute the model</p></li>
</ol>
<p>Note: Reminder to exit out of the “third-party” directory, before proceeding.</p>
<div class="section" id="step-1-export">
<h3>Step 1. Export<a class="headerlink" href="#step-1-export" title="Permalink to this heading">¶</a></h3>
<p><a class="reference external" href="https://pytorch.org/executorch/main/export-overview.html">Exporting to ExecuTorch</a> simply describes taking an existing model and converting it to the ExecuTorch format.</p>
<p>To start, let’s retrieve our model:</p>
<p><code class="docutils literal notranslate"><span class="pre">wget</span>&#160; <span class="pre">https://raw.githubusercontent.com/karpathy/nanoGPT/master/model.py</span></code></p>
<p>Next, we’ll create a script (call it export.py) to generate the ExecuTorch Program (which gets dumped into an ExecuTorch Binary):</p>
<ol class="arabic simple">
<li><p>Create the model and example inputs</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">model</span> <span class="kn">import</span> <span class="n">GPT</span>

<span class="n">model</span>  <span class="o">=</span>  <span class="n">GPT</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="n">example_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span> <span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Trace the model
Tracing extracts a cleaner representation of our model for conversion to ExecuTorch.
You can read more about tracing in <a class="reference external" href="https://pytorch.org/docs/stable/export.html">torch.export — PyTorch 2.2 documentation</a>.</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn.attention</span> <span class="kn">import</span> <span class="n">sdpa_kernel</span><span class="p">,</span>  <span class="n">SDPBackend</span>
<span class="kn">from</span> <span class="nn">torch._export</span> <span class="kn">import</span> <span class="n">capture_pre_autograd_graph</span>
<span class="kn">from</span> <span class="nn">torch.export</span> <span class="kn">import</span> <span class="n">export</span>

<span class="c1"># Using a custom SDPA kernel for LLMs</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">sdpa_kernel</span><span class="p">([</span><span class="n">SDPBackend</span><span class="o">.</span><span class="n">MATH</span><span class="p">]),</span>  <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

<span class="n">m</span>  <span class="o">=</span>  <span class="n">capture_pre_autograd_graph</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>  <span class="n">example_inputs</span><span class="p">)</span>

<span class="n">traced_model</span>  <span class="o">=</span>  <span class="n">export</span><span class="p">(</span><span class="n">m</span><span class="p">,</span>  <span class="n">example_inputs</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Export the model to ExecuTorch
Exporting (or lowering) takes the model and creates a runnable ExecuTorch program, without delegate to any specific bakends for further acceleration.</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">executorch.exir</span> <span class="kn">import</span> <span class="n">EdgeCompileConfig</span><span class="p">,</span>  <span class="n">to_edge</span>

<span class="n">edge_config</span>  <span class="o">=</span>  <span class="n">EdgeCompileConfig</span><span class="p">(</span><span class="n">_check_ir_validity</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">edge_manager</span>  <span class="o">=</span>  <span class="n">to_edge</span><span class="p">(</span><span class="n">traced_model</span><span class="p">,</span>  <span class="n">compile_config</span><span class="o">=</span><span class="n">edge_config</span><span class="p">)</span>
<span class="n">et_program</span>  <span class="o">=</span>  <span class="n">edge_manager</span><span class="o">.</span><span class="n">to_executorch</span><span class="p">()</span>
</pre></div>
</div>
<p>Also ExecuTorch provides different backend support for mobile acceleration. Simply call <code class="docutils literal notranslate"><span class="pre">to_backend()</span></code> with the specific backend partitioner on edge_manager  during exportation. Take Xnnpack delegation as an example:</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">executorch.backends.xnnpack.partition.xnnpack_partitioner</span> <span class="kn">import</span> <span class="n">XnnpackPartitioner</span>
<span class="kn">from</span> <span class="nn">executorch.backends.xnnpack.utils.configs</span> <span class="kn">import</span> <span class="n">get_xnnpack_edge_compile_config</span>
<span class="kn">from</span> <span class="nn">executorch.exir</span> <span class="kn">import</span> <span class="n">EdgeCompileConfig</span><span class="p">,</span> <span class="n">to_edge</span>

<span class="n">edge_config</span> <span class="o">=</span> <span class="n">edge_config</span> <span class="o">=</span> <span class="n">get_xnnpack_edge_compile_config</span><span class="p">()</span>
<span class="n">edge_manager</span> <span class="o">=</span> <span class="n">to_edge</span><span class="p">(</span><span class="n">traced_model</span><span class="p">,</span> <span class="n">compile_config</span><span class="o">=</span><span class="n">edge_config</span><span class="p">)</span>
<span class="n">edge_manager</span> <span class="o">=</span> <span class="n">edge_manager</span><span class="o">.</span><span class="n">to_backend</span><span class="p">(</span><span class="n">XnnpackPartitioner</span><span class="p">())</span>

<span class="n">et_program</span> <span class="o">=</span> <span class="n">edge_manager</span><span class="o">.</span><span class="n">to_executorch</span><span class="p">()</span>
</pre></div>
</div>
<p>After that, we’re ready to run our model. Remember to save you model before proceeding:</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="c1">#Write the serialized ExecuTorch program to a file.</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;nanogpt.pte&quot;</span><span class="p">,</span>  <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
<span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">et_program</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>
</pre></div>
</div>
<p>Then run the script.
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">export.py</span></code></p>
</div>
<div class="section" id="step-2-running-the-model">
<h3>Step 2. Running the model<a class="headerlink" href="#step-2-running-the-model" title="Permalink to this heading">¶</a></h3>
<p>Running model stands for executing the exported model on ExecuTorch runtime platform.</p>
<p>Before running, we need to retrieve vocabulary file GPT2 used for tokenization:</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">wget</span>  <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="o">.</span><span class="n">co</span><span class="o">/</span><span class="n">openai</span><span class="o">-</span><span class="n">community</span><span class="o">/</span><span class="n">gpt2</span><span class="o">/</span><span class="n">resolve</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">vocab</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Create the prompt:
Prompt here means the initial cue given to the model, which it uses as a starting point to generate following sentences. Here we use “Hello world!” as example:</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">string</span>  <span class="n">prompt</span>  <span class="o">=</span>  <span class="s2">&quot;Hello world!&quot;</span><span class="p">;</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Load tokenizer and model
A Tokenizer is a crucial component among different Natural Language Processing (NLP) tasks. The primary functionalities are:</p></li>
</ol>
<ul class="simple">
<li><p>Encode: Convert text into structural and numerical representations by parsing text into smaller units.Each unit is replaced by a specific number for the NLP model to consume</p></li>
<li><p>Decode: Convert the numerical representations back for human interpretation.</p></li>
</ul>
<p>In our NanoGPT example, we create a simple tokenizer called BasicTokenizer to demonstrate the function. You can use other implementations like <a class="reference external" href="https://github.com/openai/tiktoken">tiktoken</a> or your own implementation to do that.</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="c1">#include  &quot;basic_tokenizer.h&quot;</span>
<span class="n">BasicTokenizer</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;vocab.json&quot;</span><span class="p">);</span>
</pre></div>
</div>
<p>To load the exported ExecuTorch model into runtime environment, we can use <strong>Module</strong> class:</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="c1">#include &lt;executorch/extension/module/module.h&gt;</span>
<span class="n">Module</span> <span class="n">llm_model</span><span class="p">(</span><span class="s2">&quot;nanogpt.pte&quot;</span><span class="p">);</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Tokenize the prompt</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">vector</span><span class="o">&lt;</span><span class="n">int64_t</span><span class="o">&gt;</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">);</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Generate outputs
We use the loaded model to generate text based on tokenized prompt. Here we create a helper function to illustrate the pipeline:</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">vector</span><span class="o">&lt;</span><span class="n">int64_t</span><span class="o">&gt;</span> <span class="n">generate</span><span class="p">(</span><span class="n">Module</span><span class="o">&amp;</span> <span class="n">llm_model</span><span class="p">,</span> <span class="n">vector</span><span class="o">&lt;</span><span class="n">int64_t</span><span class="o">&gt;&amp;</span> <span class="n">input_tokens</span><span class="p">,</span> <span class="n">BasicSampler</span><span class="o">&amp;</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">size_t</span> <span class="n">target_output_length</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="n">int64_t</span><span class="o">&gt;</span> <span class="n">output_tokens</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="nb">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">target_output_length</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="o">//</span> <span class="n">Convert</span> <span class="n">the</span> <span class="n">input_tokens</span> <span class="kn">from</span> <span class="nn">a</span> <span class="n">vector</span> <span class="n">of</span> <span class="n">int64_t</span> <span class="n">to</span> <span class="n">EValue</span><span class="o">.</span>
        <span class="o">//</span> <span class="n">Evalue</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">unified</span> <span class="n">data</span> <span class="nb">type</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">executorch</span> <span class="n">runtime</span><span class="o">.</span>
        <span class="n">ManagedTensor</span> <span class="n">tensor_tokens</span><span class="p">(</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">data</span><span class="p">(),</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">},</span> <span class="n">ScalarType</span><span class="p">::</span><span class="n">Long</span><span class="p">);</span>
        <span class="n">vector</span><span class="o">&lt;</span><span class="n">EValue</span><span class="o">&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">tensor_tokens</span><span class="o">.</span><span class="n">get_tensor</span><span class="p">()};</span>
        <span class="o">//</span> <span class="n">Run</span> <span class="n">the</span> <span class="n">model</span> <span class="n">given</span> <span class="n">the</span> <span class="n">Evalue</span> <span class="n">inputs</span><span class="o">.</span> <span class="n">The</span> <span class="n">model</span> <span class="n">will</span> <span class="n">also</span> <span class="k">return</span> <span class="n">a</span> <span class="n">sequence</span> <span class="n">of</span> <span class="n">EValues</span> <span class="k">as</span> <span class="n">output</span><span class="o">.</span>
        <span class="n">Result</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">EValue</span><span class="o">&gt;&gt;</span> <span class="n">logits_evalue</span> <span class="o">=</span> <span class="n">llm_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
        <span class="o">//</span> <span class="n">Convert</span> <span class="n">the</span> <span class="n">output</span> <span class="kn">from</span> <span class="nn">EValue</span> <span class="n">to</span> <span class="n">a</span> <span class="n">logits</span> <span class="ow">in</span> <span class="nb">float</span><span class="o">.</span>
        <span class="n">Tensor</span> <span class="n">logits_tensor</span> <span class="o">=</span> <span class="n">logits_evalue</span><span class="o">.</span><span class="n">get</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">toTensor</span><span class="p">();</span>
        <span class="n">vector</span><span class="o">&lt;</span><span class="nb">float</span><span class="o">&gt;</span> <span class="n">logits</span><span class="p">(</span><span class="n">logits_tensor</span><span class="o">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="nb">float</span><span class="o">&gt;</span><span class="p">(),</span> <span class="n">logits_tensor</span><span class="o">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="nb">float</span><span class="o">&gt;</span><span class="p">()</span> <span class="o">+</span> <span class="n">logits_tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">());</span>
        <span class="o">//</span> <span class="n">Sample</span> <span class="n">the</span> <span class="nb">next</span> <span class="n">token</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">logits</span><span class="o">.</span>
        <span class="n">int64_t</span> <span class="n">next_token</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">logits</span><span class="p">);</span>
        <span class="o">//</span> <span class="n">Record</span> <span class="n">the</span> <span class="nb">next</span> <span class="n">token</span>
        <span class="n">output_tokens</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">next_token</span><span class="p">);</span>
        <span class="o">//</span> <span class="n">Update</span> <span class="nb">next</span> <span class="nb">input</span><span class="o">.</span>
        <span class="n">input_tokens</span><span class="o">.</span><span class="n">erase</span><span class="p">(</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">begin</span><span class="p">());</span>
        <span class="n">input_tokens</span><span class="o">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">next_token</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">output_tokens</span><span class="p">;</span>
<span class="p">}</span>

</pre></div>
</div>
<p>And in the main function, we leverage the function to generate the outputs.</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">vector</span><span class="o">&lt;</span><span class="n">int64_t</span><span class="o">&gt;</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">llm_model</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="o">/*</span><span class="n">target_output_length</span><span class="o">*/</span><span class="mi">20</span><span class="p">);</span>
</pre></div>
</div>
<p>Notice that here outputs are tokens, rather than actual natural language.</p>
<ol class="arabic simple" start="5">
<li><p>Decode the output.
We convert the generated output tokens back to natural language for better understanding:</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">string</span> <span class="n">out_str</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">);</span>
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p>Print the generated text</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s2">&quot;output: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">out_str</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="section" id="build-and-run">
<h3>Build and Run<a class="headerlink" href="#build-and-run" title="Permalink to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p>Create the Cmake file for build</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span>cmake_minimum_required(VERSION 3.19)
project(nanogpt_runner)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED True)


# Set options for executorch build.
option(EXECUTORCH_BUILD_EXTENSION_DATA_LOADER &quot;&quot; ON)
option(EXECUTORCH_BUILD_EXTENSION_MODULE &quot;&quot; ON)
option(EXECUTORCH_BUILD_XNNPACK &quot;&quot; ON)
option(EXECUTORCH_BUILD_SDK &quot;&quot; ON) # Needed for etdump

# Include the executorch subdirectory.
add_subdirectory(
    ${CMAKE_CURRENT_SOURCE_DIR}/../executorch
    ${CMAKE_BINARY_DIR}/executorch)

# include_directories(${CMAKE_CURRENT_SOURCE_DIR}/src)

add_executable(nanogpt_runner nanogpt_runner.cpp)
target_link_libraries(
    nanogpt_runner
    PRIVATE
    etdump
    extension_module
    portable_ops_lib)

</pre></div>
</div>
<p>This CMake file links the ExecuTorch codebase, along with the necessary extensions and XNNPACK modules, to the nanogpt runner.</p>
<ol class="arabic simple" start="2">
<li><p>Build the c++ environment for nanorunner</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">rm</span> <span class="o">-</span><span class="n">rf</span> <span class="n">cmake</span><span class="o">-</span><span class="n">out</span> \
  <span class="o">&amp;&amp;</span> <span class="n">mkdir</span> <span class="n">cmake</span><span class="o">-</span><span class="n">out</span> \
  <span class="o">&amp;&amp;</span> <span class="n">cd</span> <span class="n">cmake</span><span class="o">-</span><span class="n">out</span> \
  <span class="o">&amp;&amp;</span> <span class="n">cmake</span> <span class="o">..</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>With this CMake file as well as built environment iin place, you can build the nanogpt runner binary by executing the following command:</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">cmake</span> <span class="o">--</span><span class="n">build</span> <span class="n">cmake</span><span class="o">-</span><span class="n">out</span> <span class="o">--</span><span class="n">target</span> <span class="n">nanogpt_runner</span> <span class="o">-</span><span class="n">j9</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>After the build is complete, you can run the binary with this command:</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">cmake</span><span class="o">-</span><span class="n">out</span><span class="o">/</span><span class="n">nanogpt_runner</span>
</pre></div>
</div>
<p>If everything worked it should see something like this:</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span>prompt: Hello world!
output: Hello world!

I&#39;m not sure if you&#39;ve heard of the &quot;Curse of the Dragon&quot; or
</pre></div>
</div>
</div>
</div>
<div class="section" id="quantization-optional">
<h2>Quantization (Optional)<a class="headerlink" href="#quantization-optional" title="Permalink to this heading">¶</a></h2>
<p>Quantization refers to a set of techniques for running calculations and storing tensors using lower precision types. Compared to 32-bit floating point, using 8-bit integers can provide both a significant speedup and reduction in memory usage. There are many approaches to quantizing a model, varying in amount of pre-processing required, data types used, and impact on model accuracy and performance.</p>
<p>Because compute and memory are highly constrained on mobile devices, some form of quantization is necessary to ship large models on consumer electronics. In particular, large language models, such as Llama2, may require quantizing model weights to 4 bits or less.</p>
<p>Leveraging quantization requires transforming the model before export. PyTorch provides multiple quantization flows. Because we are quantizing a model for export, we need to use the PyTorch 2.0 export (pt2e) quantization API.</p>
<p>This example targets CPU acceleration using the XNNPACK delegate. As such, we need to use the XNNPACK-specific quantizer. Targeting a different backend will require use of the corresponding quantizer.</p>
<p>To use 8-bit integer dynamic quantization with the XNNPACK delegate, perform the following calls prior to calling export. This will update and annotate the computational graph to use quantized operators, where available.</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">executorch.backends.transforms.duplicate_dynamic_quant_chain</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DuplicateDynamicQuantChainPass</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.quantizer.xnnpack_quantizer</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_symmetric_quantization_config</span><span class="p">,</span>
    <span class="n">XNNPACKQuantizer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.quantize_pt2e</span> <span class="kn">import</span> <span class="n">convert_pt2e</span><span class="p">,</span> <span class="n">prepare_pt2e</span>

<span class="c1"># Use dynamic, per-channel quantization.</span>
<span class="n">xnnpack_quant_config</span> <span class="o">=</span> <span class="n">get_symmetric_quantization_config</span><span class="p">(</span>
    <span class="n">is_per_channel</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">is_dynamic</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">xnnpack_quantizer</span> <span class="o">=</span> <span class="n">XNNPACKQuantizer</span><span class="p">()</span>
<span class="n">xnnpack_quantizer</span><span class="o">.</span><span class="n">set_global</span><span class="p">(</span><span class="n">xnnpack_quant_config</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">capture_pre_autograd_graph</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span>

<span class="c1"># Annotate the model for quantization. This prepares the model for calibration.</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">prepare_pt2e</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">xnnpack_quantizer</span><span class="p">)</span>

<span class="c1"># Calibrate the model using representative inputs. This allows the quantization</span>
<span class="c1"># logic to determine the expected range of values in each tensor.</span>
<span class="n">m</span><span class="p">(</span><span class="o">*</span><span class="n">example_inputs</span><span class="p">)</span>

<span class="c1"># Perform the actual quantization.</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">convert_pt2e</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">fold_quantize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">DuplicateDynamicQuantChainPass</span><span class="p">()(</span><span class="n">m</span><span class="p">)</span>

<span class="n">traced_model</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span>

</pre></div>
</div>
<p>Additionally, add or update the to_backend() call to use XnnpackDynamicallyQuantizedPartitioner. This will instruct the lowering logic to emit the correct quantized operators.</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">executorch.backends.xnnpack.partition.xnnpack_partitioner</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">XnnpackDynamicallyQuantizedPartitioner</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">edge_manager</span> <span class="o">=</span> <span class="n">to_edge</span><span class="p">(</span><span class="n">traced_model</span><span class="p">,</span> <span class="n">compile_config</span><span class="o">=</span><span class="n">edge_config</span><span class="p">)</span>

<span class="c1"># Lower to XNNPACK using the appropriate quantized partitioner.</span>
<span class="n">edge_manager</span> <span class="o">=</span> <span class="n">edge_manager</span><span class="o">.</span><span class="n">to_backend</span><span class="p">(</span><span class="n">XnnpackDynamicallyQuantizedPartitioner</span><span class="p">())</span>

<span class="n">et_program</span> <span class="o">=</span> <span class="n">edge_manager</span><span class="o">.</span><span class="n">to_executorch</span><span class="p">()</span>
</pre></div>
</div>
<p>Finally, update the CMakeLists.txt to link the XNNPACK backend with the runner.</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">add_executable</span><span class="p">(</span><span class="n">nanogpt_runner</span> <span class="n">nanogpt_runner</span><span class="o">.</span><span class="n">cpp</span><span class="p">)</span>
<span class="n">target_link_libraries</span><span class="p">(</span>
    <span class="n">nanogpt_runner</span>
    <span class="n">PRIVATE</span>
    <span class="n">etdump</span>
    <span class="n">extension_module</span>
    <span class="n">portable_ops_lib</span>
    <span class="n">xnnpack_backend</span><span class="p">)</span> <span class="c1"># Link the XNNPACK backend</span>
</pre></div>
</div>
</div>
<div class="section" id="debugging-and-profiling">
<h2>Debugging and Profiling<a class="headerlink" href="#debugging-and-profiling" title="Permalink to this heading">¶</a></h2>
<p>After lowering a model by calling to_backend(), you might want to see what got delegated and what didn’t. We provide util functions to help you get insight on the delegation, and with such information, you can debug and maybe improve the delegation.</p>
<div class="section" id="debug-the-delegation">
<h3>Debug the Delegation<a class="headerlink" href="#debug-the-delegation" title="Permalink to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p>Get high level information
get_delegation_info gives you a summary of what happened to the model after the to_backend() call:</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">executorch.exir.backend.utils</span> <span class="kn">import</span> <span class="n">get_delegation_info</span>
<span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>

<span class="n">graph_module</span> <span class="o">=</span> <span class="n">edge_manager</span><span class="o">.</span><span class="n">exported_program</span><span class="p">()</span><span class="o">.</span><span class="n">graph_module</span>
<span class="n">delegation_info</span> <span class="o">=</span> <span class="n">get_delegation_info</span><span class="p">(</span><span class="n">graph_module</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">delegation_info</span><span class="o">.</span><span class="n">get_summary</span><span class="p">())</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">delegation_info</span><span class="o">.</span><span class="n">get_operator_delegation_dataframe</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s2">&quot;keys&quot;</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s2">&quot;fancy_grid&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>Take NanoGPT lowered to XNNPACK as an example:</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">Total</span>  <span class="n">delegated</span>  <span class="n">subgraphs</span><span class="p">:</span>  <span class="mi">86</span>
<span class="n">Number</span>  <span class="n">of</span>  <span class="n">delegated</span>  <span class="n">nodes</span><span class="p">:</span>  <span class="mi">473</span>
<span class="n">Number</span>  <span class="n">of</span>  <span class="n">non</span><span class="o">-</span><span class="n">delegated</span>  <span class="n">nodes</span><span class="p">:</span>  <span class="mi">430</span>
</pre></div>
</div>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>op_type</p></th>
<th class="head"><p>occurrences_in_delegated_graphs</p></th>
<th class="head"><p>occurrences_in_non_delegated_graphs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>aten__softmax_default</p></td>
<td><p>12</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>aten_add_tensor</p></td>
<td><p>37</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>aten_addmm_default</p></td>
<td><p>48</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>aten_arange_start_step</p></td>
<td><p>0</p></td>
<td><p>25</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>aten_bmm_default</p></td>
<td><p>24</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>5</p></td>
<td><p>aten_clone_default</p></td>
<td><p>0</p></td>
<td><p>38</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>aten_embedding_default</p></td>
<td><p>0</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>7</p></td>
<td><p>aten_expand_copy_default</p></td>
<td><p>48</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p>aten_full_default</p></td>
<td><p>0</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p>aten_full_like_default</p></td>
<td><p>0</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-even"><td><p>10</p></td>
<td><p>aten_gelu_default</p></td>
<td><p>0</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-odd"><td><p>11</p></td>
<td><p>aten_index_tensor</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>12</p></td>
<td><p>aten_le_scalar</p></td>
<td><p>0</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-odd"><td><p>13</p></td>
<td><p>aten_logical_and_default</p></td>
<td><p>0</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-even"><td><p>14</p></td>
<td><p>aten_logical_not_default</p></td>
<td><p>0</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-odd"><td><p>15</p></td>
<td><p>aten_mm_default</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>16</p></td>
<td><p>aten_mul_scalar</p></td>
<td><p>24</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>17</p></td>
<td><p>aten_native_layer_norm_default</p></td>
<td><p>0</p></td>
<td><p>25</p></td>
</tr>
<tr class="row-even"><td><p>18</p></td>
<td><p>aten_permute_copy_default</p></td>
<td><p>109</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>19</p></td>
<td><p>aten_scalar_tensor_default</p></td>
<td><p>0</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-even"><td><p>20</p></td>
<td><p>aten_split_with_sizes_copy_default</p></td>
<td><p>0</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-odd"><td><p>21</p></td>
<td><p>aten_sub_tensor</p></td>
<td><p>0</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-even"><td><p>22</p></td>
<td><p>aten_unsqueeze_copy_default</p></td>
<td><p>0</p></td>
<td><p>24</p></td>
</tr>
<tr class="row-odd"><td><p>23</p></td>
<td><p>aten_view_copy_default</p></td>
<td><p>170</p></td>
<td><p>48</p></td>
</tr>
<tr class="row-even"><td><p>24</p></td>
<td><p>aten_where_self</p></td>
<td><p>0</p></td>
<td><p>12</p></td>
</tr>
<tr class="row-odd"><td><p>25</p></td>
<td><p>getitem</p></td>
<td><p>0</p></td>
<td><p>147</p></td>
</tr>
<tr class="row-even"><td><p>26</p></td>
<td><p>Total</p></td>
<td><p>473</p></td>
<td><p>430</p></td>
</tr>
</tbody>
</table>
<p>In the table, we see that op type aten_view_copy_default appears 170 times in delegate graphs and 48 times in non-delegated graphs.</p>
<p>| 23 | aten_view_copy_default | 170 | 48 |</p>
<p>From here, we might want to know in which part of the graph it wasn’t delegated. For that, you can use the <code class="docutils literal notranslate"><span class="pre">print_delegated_graph</span></code> util function to see a printout of the whole graph with highlighted lowered graphs.</p>
<ol class="arabic simple" start="2">
<li><p>Print graph module
Call this function right after you call <code class="docutils literal notranslate"><span class="pre">to_backend()</span></code></p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">executorch.exir.backend.utils</span> <span class="kn">import</span> <span class="n">print_delegated_graph</span>
<span class="n">graph_module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_manager</span><span class="o">.</span><span class="n">exported_program</span><span class="p">()</span><span class="o">.</span><span class="n">graph_module</span>
<span class="nb">print</span><span class="p">(</span><span class="n">print_delegated_graph</span><span class="p">(</span><span class="n">graph_module</span><span class="p">))</span>
</pre></div>
</div>
<p>On the printed graph, you can do “Control+F” (or “Command+F” on a Mac) on the operator type you’re interested in (e.g. “aten_view_copy_default”) and observe which ones of them are not under “lowered graph()”s.</p>
</div>
<div class="section" id="performance-analysis-optional">
<h3>Performance Analysis (Optional)<a class="headerlink" href="#performance-analysis-optional" title="Permalink to this heading">¶</a></h3>
<p>Through the ExecuTorch SDK, users are able to profile a model and inspect its latency performance.</p>
<div class="section" id="id1">
<h4>Prerequisites<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h4>
<div class="section" id="etrecord-generation-optional">
<h5>ETRecord generation (Optional)<a class="headerlink" href="#etrecord-generation-optional" title="Permalink to this heading">¶</a></h5>
<p>ETRecord contains model graphs and metadata for linking runtime results (such as profiling) to the eager model. You will be able to view all profiling events with just ETDump (see next section), but with ETRecord, you will also be able to link each event to the types of operators being executed, module hierarchy, and stack traces of the original PyTorch source code. For more information, see <a class="reference external" href="https://pytorch.org/executorch/main/sdk-etrecord.html">https://pytorch.org/executorch/main/sdk-etrecord.html</a></p>
<p><strong>Steps for enablement:</strong>
ETRecord is created during export. In your export script, you just called <code class="docutils literal notranslate"><span class="pre">to_edge()</span> </code>and it returned edge_program_manager</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span>

<span class="c1"># Make the deep copy right after your call to to_edge()</span>
<span class="n">edge_program_manager_copy</span>  <span class="o">=</span>  <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">edge_program_manager</span><span class="p">)</span>

<span class="c1"># ...</span>
<span class="c1"># Then generate ETRecord right after your call to to_executorch()</span>
<span class="n">etrecord_path</span>  <span class="o">=</span>  <span class="s2">&quot;etrecord.bin&quot;</span>
<span class="n">generate_etrecord</span><span class="p">(</span><span class="n">etrecord_path</span><span class="p">,</span>  <span class="n">edge_program_manager_copy</span><span class="p">,</span>  <span class="n">et_program_manager</span><span class="p">)</span>
</pre></div>
</div>
<p>Run the export script, then the ETRecord should be generated under path ./etrecord.bin.</p>
</div>
<div class="section" id="etdump-generation">
<h5>ETDump generation<a class="headerlink" href="#etdump-generation" title="Permalink to this heading">¶</a></h5>
<p>ETDump contains runtime results from executing an ExecuTorch model. For more information, see <a class="reference external" href="https://pytorch.org/executorch/main/sdk-etdump.html">https://pytorch.org/executorch/main/sdk-etdump.html</a></p>
<p><strong>Steps for enablement:</strong>
You need to enable ETDump generation in your nanogpt_runner.cpp.</p>
<ol class="arabic simple">
<li><p>Include the ETDump header in your code.</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="c1">#include  &lt;executorch/sdk/etdump/etdump_flatcc.h&gt;</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Create an Instance of the ETDumpGen class and pass it into the Module constructor</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">std</span><span class="p">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">torch</span><span class="p">::</span><span class="n">executor</span><span class="p">::</span><span class="n">ETDumpGen</span><span class="o">&gt;</span> <span class="n">etdump_gen_</span> <span class="o">=</span> <span class="n">std</span><span class="p">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">torch</span><span class="p">::</span><span class="n">executor</span><span class="p">::</span><span class="n">ETDumpGen</span><span class="o">&gt;</span><span class="p">();</span>
<span class="n">Module</span> <span class="n">llm_model</span><span class="p">(</span><span class="s2">&quot;nanogpt.pte&quot;</span><span class="p">,</span> <span class="n">Module</span><span class="p">::</span><span class="n">MlockConfig</span><span class="p">::</span><span class="n">UseMlock</span><span class="p">,</span> <span class="n">std</span><span class="p">::</span><span class="n">move</span><span class="p">(</span><span class="n">etdump_gen_</span><span class="p">));</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Dump out the ETDump buffer after call to generate()</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="p">::</span><span class="n">executor</span><span class="p">::</span><span class="n">ETDumpGen</span><span class="o">*</span> <span class="n">etdump_gen</span> <span class="o">=</span>
<span class="n">static_cast</span><span class="o">&lt;</span><span class="n">torch</span><span class="p">::</span><span class="n">executor</span><span class="p">::</span><span class="n">ETDumpGen</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">llm_model</span><span class="o">.</span><span class="n">event_tracer</span><span class="p">());</span>

<span class="n">ET_LOG</span><span class="p">(</span><span class="n">Info</span><span class="p">,</span> <span class="s2">&quot;ETDump size: %zu blocks&quot;</span><span class="p">,</span> <span class="n">etdump_gen</span><span class="o">-&gt;</span><span class="n">get_num_blocks</span><span class="p">());</span>
<span class="n">etdump_result</span> <span class="n">result</span> <span class="o">=</span> <span class="n">etdump_gen</span><span class="o">-&gt;</span><span class="n">get_etdump_data</span><span class="p">();</span>
<span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">buf</span> <span class="o">!=</span> <span class="n">nullptr</span> <span class="o">&amp;&amp;</span> <span class="n">result</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
<span class="o">//</span> <span class="n">On</span> <span class="n">a</span> <span class="n">device</span> <span class="k">with</span> <span class="n">a</span> <span class="n">file</span> <span class="n">system</span> <span class="n">users</span> <span class="n">can</span> <span class="n">just</span> <span class="n">write</span> <span class="n">it</span> <span class="n">out</span>
<span class="o">//</span> <span class="n">to</span> <span class="n">the</span> <span class="n">file</span><span class="o">-</span><span class="n">system</span><span class="o">.</span>
<span class="n">FILE</span><span class="o">*</span> <span class="n">f</span> <span class="o">=</span> <span class="n">fopen</span><span class="p">(</span><span class="s2">&quot;etdump.etdp&quot;</span><span class="p">,</span> <span class="s2">&quot;w+&quot;</span><span class="p">);</span>
<span class="n">fwrite</span><span class="p">((</span><span class="n">uint8_t</span><span class="o">*</span><span class="p">)</span><span class="n">result</span><span class="o">.</span><span class="n">buf</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">f</span><span class="p">);</span>
<span class="n">fclose</span><span class="p">(</span><span class="n">f</span><span class="p">);</span>
<span class="n">free</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">buf</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Compile your binary with the <code class="docutils literal notranslate"><span class="pre">ET_EVENT_TRACER_ENABLED</span></code> pre-processor flag to enable events to be traced and logged into ETDump inside the ExecuTorch runtime. Add these to your CMakeLists.txt</p></li>
</ol>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="n">target_compile_options</span><span class="p">(</span><span class="n">executorch</span> <span class="n">PUBLIC</span> <span class="o">-</span><span class="n">DET_EVENT_TRACER_ENABLED</span><span class="p">)</span>
<span class="n">target_compile_options</span><span class="p">(</span><span class="n">portable_ops_lib</span> <span class="n">PUBLIC</span> <span class="o">-</span><span class="n">DET_EVENT_TRACER_ENABLED</span><span class="p">)</span>
</pre></div>
</div>
<p>Run the runner, you will see “etdump.etdp” generated.</p>
</div>
</div>
<div class="section" id="analyze-with-inspector-apis">
<h4>Analyze with Inspector APIs<a class="headerlink" href="#analyze-with-inspector-apis" title="Permalink to this heading">¶</a></h4>
<p>Once you’ve collected debug artifacts ETDump (and the optional ETRecord), you can feed them into Inspector APIs in order to get performance details.</p>
<div class="section" id="creating-an-inspector">
<h5>Creating an Inspector<a class="headerlink" href="#creating-an-inspector" title="Permalink to this heading">¶</a></h5>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">executorch.sdk</span> <span class="kn">import</span> <span class="n">Inspector</span>

<span class="n">inspector</span> <span class="o">=</span> <span class="n">Inspector</span><span class="p">(</span><span class="n">etdump_path</span><span class="o">=</span><span class="s2">&quot;etdump.etdp&quot;</span><span class="p">,</span> <span class="n">etrecord</span><span class="o">=</span><span class="s2">&quot;etrecord.bin&quot;</span><span class="p">)</span>
<span class="c1"># If you did not generate an ETRecord, then just pass in ETDump: `inspector = Inspector(etdump_path=&quot;etdump.etdp&quot;)`</span>
</pre></div>
</div>
<p>Using an Inspector</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="k">with</span>  <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;inspector_out.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">inspector</span><span class="o">.</span><span class="n">print_data_tabular</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
</pre></div>
</div>
<p>This saves the performance data in a tabular format in “inspector_out.txt”, with each row being a profiling event. Top rows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>event_block_name</p></th>
<th class="head"><p>event_name</p></th>
<th class="head"><p>p10  (ms)</p></th>
<th class="head"><p>p50  (ms)</p></th>
<th class="head"><p>p90  (ms)</p></th>
<th class="head"><p>avg  (ms)</p></th>
<th class="head"><p>min  (ms)</p></th>
<th class="head"><p>max  (ms)</p></th>
<th class="head"><p>op_types</p></th>
<th class="head"><p>is_delegated_op</p></th>
<th class="head"><p>delegate_backend_name</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>Default</p></td>
<td><p>Method::init</p></td>
<td><p>60.502</p></td>
<td><p>60.502</p></td>
<td><p>60.502</p></td>
<td><p>60.502</p></td>
<td><p>60.502</p></td>
<td><p>60.502</p></td>
<td><p>[]</p></td>
<td><p>False</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Default</p></td>
<td><p>Program::load_method</p></td>
<td><p>60.5114</p></td>
<td><p>60.5114</p></td>
<td><p>60.5114</p></td>
<td><p>60.5114</p></td>
<td><p>60.5114</p></td>
<td><p>60.5114</p></td>
<td><p>[]</p></td>
<td><p>False</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>Execute</p></td>
<td><p>native_call_arange.start_out</p></td>
<td><p>0.029583</p></td>
<td><p>0.029583</p></td>
<td><p>0.029583</p></td>
<td><p>0.029583</p></td>
<td><p>0.029583</p></td>
<td><p>0.029583</p></td>
<td><p>[]</p></td>
<td><p>False</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>Execute</p></td>
<td><p>native_call_embedding.out</p></td>
<td><p>0.022916</p></td>
<td><p>0.022916</p></td>
<td><p>0.022916</p></td>
<td><p>0.022916</p></td>
<td><p>0.022916</p></td>
<td><p>0.022916</p></td>
<td><p>[]</p></td>
<td><p>False</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Execute</p></td>
<td><p>native_call_embedding.out</p></td>
<td><p>0.001084</p></td>
<td><p>0.001084</p></td>
<td><p>0.001084</p></td>
<td><p>0.001084</p></td>
<td><p>0.001084</p></td>
<td><p>0.001084</p></td>
<td><p>[]</p></td>
<td><p>False</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>For more information about Inspector APIs and the rich functionality it provides, see <a class="reference external" href="https://pytorch.org/executorch/main/sdk-inspector.html">https://pytorch.org/executorch/main/sdk-inspector.html</a>.</p>
</div>
</div>
</div>
</div>
<div class="section" id="how-to-use-custom-kernels">
<h2>How to use custom kernels<a class="headerlink" href="#how-to-use-custom-kernels" title="Permalink to this heading">¶</a></h2>
<p>With our new custom op APIs, custom op/kernel authors can easily bring in their op/kernel into PyTorch/ExecuTorch and the process is streamlined.</p>
<p>There are three steps to use custom kernels in ExecuTorch:</p>
<ol class="arabic simple">
<li><p>Prepare the kernel implementation using ExecuTorch types.</p></li>
<li><p>Compile and link the custom kernel to both AOT Python environment as well as the runner binary.</p></li>
<li><p>Source-to-source transformation to swap an operator with a custom op.</p></li>
</ol>
<div class="section" id="prepare-custom-kernel-implementation">
<h3>Prepare custom kernel implementation<a class="headerlink" href="#prepare-custom-kernel-implementation" title="Permalink to this heading">¶</a></h3>
<p>Define your custom operator schema for both functional variant (used in AOT compilation) and out variant (used in ExecuTorch runtime). The schema needs to follow PyTorch ATen convention (see <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml">native_functions.yaml</a>). For example:</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span>custom_linear(Tensor weight, Tensor input, Tensor(?) bias) -&gt; Tensor

custom_linear.out(Tensor weight, Tensor input, Tensor(?) bias, *, Tensor(a!) out) -&gt; Tensor(a!)
</pre></div>
</div>
<p>Then write your custom kernel according to the schema using ExecuTorch types, along with APIs to register to ExecuTorch runtime:</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">custom_linear</span><span class="o">.</span><span class="n">h</span><span class="o">/</span><span class="n">custom_linear</span><span class="o">.</span><span class="n">cpp</span>
<span class="c1">#include &lt;executorch/runtime/kernel/kernel_includes.h&gt;</span>

<span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">custom_linear_out</span><span class="p">(</span><span class="n">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">weight</span><span class="p">,</span> <span class="n">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="nb">input</span><span class="p">,</span> <span class="n">optional</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">bias</span><span class="p">,</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">out</span><span class="p">)</span> <span class="p">{</span>

<span class="o">//</span> <span class="n">calculation</span>
<span class="k">return</span> <span class="n">out</span><span class="p">;</span>
<span class="p">}</span>

<span class="o">//</span> <span class="n">opset</span> <span class="n">namespace</span> <span class="n">myop</span>
<span class="n">EXECUTORCH_LIBRARY</span><span class="p">(</span><span class="n">myop</span><span class="p">,</span> <span class="s2">&quot;custom_linear.out&quot;</span><span class="p">,</span> <span class="n">custom_linear_out</span><span class="p">);</span>
</pre></div>
</div>
<p>Now we need to write some wrapper for this op to show up in PyTorch, but don’t worry we don’t need to rewrite the kernel. Create a separate .cpp for this purpose:</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="o">//</span> <span class="n">custom_linear_pytorch</span><span class="o">.</span><span class="n">cpp</span>
<span class="c1">#include &quot;custom_linear.h&quot;</span>
<span class="c1">#include &lt;torch/library.h&gt;</span>

<span class="n">at</span><span class="p">::</span><span class="n">Tensor</span> <span class="n">custom_linear</span><span class="p">(</span><span class="n">const</span> <span class="n">at</span><span class="p">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">weight</span><span class="p">,</span> <span class="n">const</span> <span class="n">at</span><span class="p">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="nb">input</span><span class="p">,</span> <span class="n">std</span><span class="p">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="p">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">bias</span><span class="p">)</span> <span class="p">{</span>

<span class="o">//</span> <span class="n">initialize</span> <span class="n">out</span>
<span class="n">at</span><span class="p">::</span><span class="n">Tensor</span> <span class="n">out</span> <span class="o">=</span> <span class="n">at</span><span class="p">::</span><span class="n">empty</span><span class="p">({</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)});</span>

<span class="o">//</span> <span class="n">wrap</span> <span class="n">kernel</span> <span class="ow">in</span> <span class="n">custom_linear</span><span class="o">.</span><span class="n">cpp</span> <span class="n">into</span> <span class="n">ATen</span> <span class="n">kernel</span>
<span class="n">WRAP_TO_ATEN</span><span class="p">(</span><span class="n">custom_linear_out</span><span class="p">,</span> <span class="mi">3</span><span class="p">)(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">out</span><span class="p">);</span>

<span class="k">return</span> <span class="n">out</span><span class="p">;</span>
<span class="p">}</span>

<span class="o">//</span> <span class="n">standard</span> <span class="n">API</span> <span class="n">to</span> <span class="n">register</span> <span class="n">ops</span> <span class="n">into</span> <span class="n">PyTorch</span>
<span class="n">TORCH_LIBRARY</span><span class="p">(</span><span class="n">myop</span><span class="p">,</span>  <span class="n">m</span><span class="p">)</span> <span class="p">{</span>

<span class="n">m</span><span class="o">.</span><span class="n">def</span><span class="p">(</span><span class="s2">&quot;custom_linear(Tensor weight, Tensor input, Tensor(?) bias) -&gt; Tensor&quot;</span><span class="p">,</span> <span class="n">custom_linear</span><span class="p">);</span>

<span class="n">m</span><span class="o">.</span><span class="n">def</span><span class="p">(</span><span class="s2">&quot;custom_linear.out(Tensor weight, Tensor input, Tensor(?) bias, *, Tensor(a!) out) -&gt; Tensor(a!)&quot;</span><span class="p">,</span> <span class="n">WRAP_TO_ATEN</span><span class="p">(</span><span class="n">custom_linear_out</span><span class="p">,</span> <span class="mi">3</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="compile-and-link-the-custom-kernel">
<h3>Compile and link the custom kernel<a class="headerlink" href="#compile-and-link-the-custom-kernel" title="Permalink to this heading">¶</a></h3>
<p>Link it into ExecuTorch runtime: In our runner CMakeLists.txt we just need to add custom_linear.h/cpp into the binary target. We can build a dynamically loaded library (.so or .dylib) and link it as well.</p>
<p>Link it into PyTorch runtime: We need to package custom_linear.h, custom_linear.cpp and custom_linear_pytorch.cpp into a dynamically loaded library (.so or .dylib) and load it into our python environment. One way of doing this is:</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">load_library</span><span class="p">(</span><span class="s2">&quot;libcustom_linear.so/dylib&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Once loaded we can perform the next step, of introducing the custom op into PyTorch environment.</p>
</div>
<div class="section" id="source-to-source-transformation-to-introduce-the-custom-op">
<h3>Source-to-source transformation to introduce the custom op<a class="headerlink" href="#source-to-source-transformation-to-introduce-the-custom-op" title="Permalink to this heading">¶</a></h3>
<p>Easier way to introduce our customized linear is by rewriting the eager model. However, that may miss some occurrences of torch.nn.Linear in our example. A safer option is to walk through all the modules in the module hierarchy and perform the swapping.</p>
<p>For example, we can do the following to swap torch.nn.Linear with our custom linear op:</p>
<div class="highlight-none notranslate highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span>  <span class="nf">replace_linear_with_custom_linear</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
    <span class="k">for</span>  <span class="n">name</span><span class="p">,</span>  <span class="n">child</span>  <span class="ow">in</span>  <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">if</span>  <span class="nb">isinstance</span><span class="p">(</span><span class="n">child</span><span class="p">,</span>  <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="nb">setattr</span><span class="p">(</span>
                <span class="n">module</span><span class="p">,</span>
                <span class="n">name</span><span class="p">,</span>
                <span class="n">CustomLinear</span><span class="p">(</span><span class="n">child</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span>  <span class="n">child</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">child</span><span class="o">.</span><span class="n">bias</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">replace_linear_with_custom_linear</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
</pre></div>
</div>
<p>The rest of the steps will be the same as the normal flow. Now you can run this module in eager as well as export it to ExecuTorch and run on the runner.</p>
</div>
</div>
<div class="section" id="how-to-build-mobile-apps">
<h2>How to build Mobile Apps<a class="headerlink" href="#how-to-build-mobile-apps" title="Permalink to this heading">¶</a></h2>
<p>You can also execute an LLM using ExecuTorch on iOS and Android</p>
<p><strong>For iOS see the <a class="reference external" href="https://pytorch.org/executorch/main/llm/llama-demo-ios.html">iLLaMA App</a>.</strong></p>
<p><strong>For Android see the <a class="reference external" href="https://pytorch.org/executorch/main/llm/llama-demo-android.html">Android Instructions</a>.</strong></p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../tutorials/export-to-executorch-tutorial.html" class="btn btn-neutral float-right" title="Exporting to ExecuTorch Tutorial" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="../runtime-build-and-cross-compilation.html" class="btn btn-neutral" title="Building with CMake" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024, ExecuTorch.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Getting Started with LLMs via ExecuTorch</a><ul>
<li><a class="reference internal" href="#table-of-contents">Table Of Contents</a></li>
<li><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li><a class="reference internal" href="#instantiating-and-executing-an-llm">Instantiating and Executing an LLM</a><ul>
<li><a class="reference internal" href="#step-1-export">Step 1. Export</a></li>
<li><a class="reference internal" href="#step-2-running-the-model">Step 2. Running the model</a></li>
<li><a class="reference internal" href="#build-and-run">Build and Run</a></li>
</ul>
</li>
<li><a class="reference internal" href="#quantization-optional">Quantization (Optional)</a></li>
<li><a class="reference internal" href="#debugging-and-profiling">Debugging and Profiling</a><ul>
<li><a class="reference internal" href="#debug-the-delegation">Debug the Delegation</a></li>
<li><a class="reference internal" href="#performance-analysis-optional">Performance Analysis (Optional)</a><ul>
<li><a class="reference internal" href="#id1">Prerequisites</a><ul>
<li><a class="reference internal" href="#etrecord-generation-optional">ETRecord generation (Optional)</a></li>
<li><a class="reference internal" href="#etdump-generation">ETDump generation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#analyze-with-inspector-apis">Analyze with Inspector APIs</a><ul>
<li><a class="reference internal" href="#creating-an-inspector">Creating an Inspector</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-use-custom-kernels">How to use custom kernels</a><ul>
<li><a class="reference internal" href="#prepare-custom-kernel-implementation">Prepare custom kernel implementation</a></li>
<li><a class="reference internal" href="#compile-and-link-the-custom-kernel">Compile and link the custom kernel</a></li>
<li><a class="reference internal" href="#source-to-source-transformation-to-introduce-the-custom-op">Source-to-source transformation to introduce the custom op</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-build-mobile-apps">How to build Mobile Apps</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/sphinx_highlight.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/design-tabs.js"></script>
         <script src="../_static/js/progress-bar.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script>
 
<script script type="text/javascript">
  var collapsedSections = ['Introduction', 'Getting Started', 'Working with LLMs', 'Exporting to ExecuTorch',  'API Reference', 'IR Specification', 'Compiler Entry Points', 'Runtime', 'Quantization', 'Kernel Library', 'Native Delegates', 'Backend Delegates', 'SDK', 'Tutorials']
</script>

 
<script type="text/javascript">
// Handle the right navigation in third level pages. Without this
// in third level, only the last item always selected. This is a hacky
// way and we should revise it eventually.
// #side-scroll-highlight is disabled in .css.
// Get all menu items
var menuItems = document.querySelectorAll('.pytorch-right-menu a.reference.internal');
// Add a click event listener to each menu item
for (var i = 0; i < menuItems.length; i++) {
  menuItems[i].addEventListener('click', function(event) {
    // Remove the 'side-scroll-highlight-local' class from all menu items
    for (var j = 0; j < menuItems.length; j++) {
      menuItems[j].classList.remove('side-scroll-highlight-local');
    }
    // Add the 'side-scroll-highlight-local' class to the clicked item
    event.target.classList.add('side-scroll-highlight-local');
  });
}
</script>

 
<script type="text/javascript">
  $(document).ready(function () {
    // Patch links on interactive tutorial pages to point
    // to the correct ExecuTorch URLs.
    var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
    if (downloadNote.length >= 1) {
      var tutorialUrl = $("#tutorial-type").text().substring($("#tutorial-type").text().indexOf("tutorials/") + 9); // 9 is the length of "tutorials/"
      var githubLink = "https://github.com/pytorch/executorch/blob/main/docs/source/tutorials_source" + tutorialUrl + ".py",
        notebookLink = $(".reference.download")[1].href,
        notebookDownloadPath = notebookLink.split('_downloads')[1],
        colabLink = "https://colab.research.google.com/github/pytorch/executorch/blob/gh-pages/main/_downloads" + notebookDownloadPath;

      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
    }

    // Patch the "GitHub" link at the top of the page
    // to point to the ExecuTorch repo.
    var overwrite = function (_) {
      if ($(this).length > 0) {
        $(this)[0].href = "https://github.com/pytorch/executorch"
      }
    }
    // PC
    $(".main-menu a:contains('GitHub')").each(overwrite);
    // Overwrite link to Tutorials and Get Started top navigation. If these sections are moved
    // this overrides need to be updated.
    $(".main-menu a:contains('Tutorials')").attr("href", "https://pytorch.org/executorch/stable/index.html#tutorials-and-examples");
    $(".main-menu a:contains('Get Started')").attr("href", "https://pytorch.org/executorch/stable/getting-started-setup.html");
    // Mobile
    $(".mobile-menu a:contains('Github')").each(overwrite);
    // Overwrite link to Tutorials and Get Started top navigation. If these sections are moved
    // this overrides need to be updated.
    $(".mobile-menu a:contains('Tutorials')").attr("href", "https://pytorch.org/executorch/stable/index.html#tutorials-and-examples");
    $(".mobile-menu a:contains('Get Started')").attr("href", "https://pytorch.org/executorch/stable/getting-started-setup.html");

  });
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>